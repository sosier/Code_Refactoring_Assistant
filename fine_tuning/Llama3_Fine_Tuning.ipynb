{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from eval import DATA\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else (\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "RESULTS_PATH = \"./results/\"\n",
    "MODELS_PATH = os.getenv(\"HOME\") + \"/models/fine_tuned/llama3/\"\n",
    "\n",
    "mbpp = DATA[\"mbpp\"]  # train, validation, and test\n",
    "humaneval = DATA[\"openai_humaneval\"]  # test only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(object, to):\n",
    "    with open(to, \"wb\") as f:\n",
    "        pickle.dump(object, f)\n",
    "\n",
    "def load_pickle(from_):\n",
    "    with open(from_, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_canonical_solutions(dataset, split):\n",
    "    assert dataset in (\"mbpp\", \"openai_humaneval\")\n",
    "\n",
    "    if dataset == \"mbpp\":\n",
    "        mbpp = DATA[\"mbpp\"]\n",
    "        return [\n",
    "            \"# \" + task[\"text\"] + \"\\n\" + task[\"code\"] for task in mbpp[split]\n",
    "        ]\n",
    "    else:  # humaneval\n",
    "        humaneval = DATA[\"openai_humaneval\"]\n",
    "        return [\n",
    "            task[\"prompt\"] + task[\"canonical_solution\"] for task in humaneval[\"test\"]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea0ce3128ec4863b661a9299987a6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 3\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # NOTE: BFloat16 is not supported on MPS, so using Float16\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Time (One Time Only: No Need to Run This Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_solutions = get_canonical_solutions(\"mbpp\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH + \"mbpp_train_claude_3_haiku_0_shot_v4_prompt.csv\", index_col=0)\n",
    "assert len(canonical_solutions) == len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed_df = df.query(\"passed_tests\")\n",
    "passed_code = dict(zip(passed_df[\"task_id\"], passed_df[\"code\"]))\n",
    "print(\"n =\", len(passed_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the world's best AI coding assistant. In particular, you are exceptionally skilled at refactoring Python programs to be readable, efficient, and maintainable.\n",
    "\n",
    "For the interaction that follows, refactor the Python code provided by the user to be more readable, efficient, and maintainable using the following guidelines:\n",
    " - The given program is correct but needs improvement\n",
    " - DO NOT change the name of the program\n",
    " - DO NOT change the input or output behavior of the program (e.g. number of inputs / outputs, input / output types, etc.)\n",
    " - Put your response in a markdown code block\n",
    " - Respond with only the code block\n",
    " - Don't explain the changes made\n",
    " - If you use any packages (e.g. `os`, `re`, `sys`), don't forget to import them\n",
    "\n",
    "Again, do not change the name of the function in any way!\n",
    "\"\"\".strip()\n",
    "\n",
    "CODE_BLOCK = \"\"\"\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "def format_as_prompt(system_prompt, user_prompt, model_response):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": model_response}\n",
    "    ]\n",
    "\n",
    "    return pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )[:-len(\"<|start_header_id|>assistant<|end_header_id|>  \")]\n",
    "\n",
    "print(\n",
    "    format_as_prompt(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=CODE_BLOCK.format(code=canonical_solutions[0]),\n",
    "        model_response=CODE_BLOCK.format(code=df[\"code\"][0])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response_strings = [\n",
    "    format_as_prompt(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=CODE_BLOCK.format(code=canonical_solutions[i]),\n",
    "        model_response=CODE_BLOCK.format(code=code)\n",
    "    )\n",
    "    for i, code in passed_code.items()\n",
    "]\n",
    "print(len(query_response_strings))\n",
    "print(query_response_strings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_examples = pd.DataFrame({\"example\": query_response_strings})\n",
    "fine_tuning_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tuning_examples.to_csv(RESULTS_PATH + \"fine_tuning_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               example\n",
       "0    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "1    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "2    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "3    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "4    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "..                                                 ...\n",
       "276  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "277  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "278  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "279  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "280  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "\n",
       "[281 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_examples = pd.read_csv(RESULTS_PATH + \"fine_tuning_examples.csv\")\n",
    "fine_tuning_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence_length=740\n",
      "torch.Size([281, 740])\n",
      "torch.Size([281, 739])\n",
      "torch.Size([281, 739])\n"
     ]
    }
   ],
   "source": [
    "X = fine_tuning_examples[\"example\"].apply(\n",
    "    lambda example:\n",
    "        pipeline.tokenizer.encode(example)[1:]\n",
    "        + [pipeline.tokenizer.eos_token_id]\n",
    ").apply(torch.tensor)\n",
    "\n",
    "max_sequence_length = X.apply(len).max()\n",
    "print(f\"{max_sequence_length=}\")\n",
    "\n",
    "# Pad sequences with EOS token\n",
    "X = X.apply(\n",
    "    lambda vec:\n",
    "        torch.cat([\n",
    "            vec,\n",
    "            torch.tensor(\n",
    "                [pipeline.tokenizer.eos_token_id]\n",
    "                * (max_sequence_length - len(vec))\n",
    "            )\n",
    "        ])\n",
    "        if len(vec) < max_sequence_length\n",
    "        else vec\n",
    ")\n",
    "# Covert entire list into one tensor\n",
    "X = torch.stack(X.tolist())\n",
    "\n",
    "print(X.shape)\n",
    "Y = X[:, 1:]\n",
    "X = X[:, :-1]\n",
    "\n",
    "X = X.to(DEVICE)\n",
    "Y = Y.to(DEVICE)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with torch DataLoader:\n",
    "class TrainingData(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        global DEVICE\n",
    "        assert X.shape == Y.shape\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "\n",
    "def _next_X_y(data_loader):\n",
    "    global DEVICE\n",
    "    X, y = next(iter(data_loader))\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    return X, y\n",
    "\n",
    "def _train_on_batch(model, X, y, optimizer):\n",
    "    # Put model into training mode:\n",
    "    model.train()\n",
    "\n",
    "    # Do forward pass and evaluate loss\n",
    "    loss = model(X, labels=y).loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def _train_for_n_batches(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    optimizer,\n",
    "    batches_to_run,\n",
    "    verbose,\n",
    "    print_every\n",
    "):\n",
    "    batches_run = 0\n",
    "    for _ in range(batches_to_run):\n",
    "        X, y = _next_X_y(train_data_loader)\n",
    "        loss = _train_on_batch(model, X, y, optimizer)\n",
    "        batches_run += 1\n",
    "\n",
    "        # Reporting\n",
    "        if verbose and batches_run % print_every == 0:\n",
    "            print(f\"Batch {batches_run}: loss =\", loss.item())\n",
    "\n",
    "def _train_for_n_epoches(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    optimizer,\n",
    "    epochs_to_run,\n",
    "    verbose,\n",
    "    print_every,\n",
    "    save_after_epoch=False,\n",
    "    save_as=\"model.pt\",\n",
    "    starting_epoch=1\n",
    "):\n",
    "    global DEVICE\n",
    "    batches_run = 0\n",
    "    losses = []\n",
    "    for i in range(epochs_to_run):\n",
    "        # For each batch:\n",
    "        for X, y in train_data_loader:\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            loss = _train_on_batch(model, X, y, optimizer)\n",
    "            batches_run += 1\n",
    "            losses.append(loss.item())\n",
    "            losses = losses[-1000:]  # Keep max 1000 losses\n",
    "\n",
    "            # Reporting\n",
    "            if verbose and batches_run % print_every == 0:\n",
    "                print(f\"Batch {batches_run}: loss =\", loss.item())\n",
    "\n",
    "        # Print after every epoch\n",
    "        if i == 0:\n",
    "            print(\"=\" * 20)\n",
    "        print(f\"After {i + starting_epoch} epoch(s):\")\n",
    "        print(\"  loss =\", np.mean(losses[-30:]))\n",
    "\n",
    "        if save_after_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                save_as.format(epoch_number=i + starting_epoch)\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_data,\n",
    "    optimizer=\"Adam\",\n",
    "    epochs_to_run=None,  # Train for 1 epoch if no training limit is given\n",
    "    batches_to_run=None,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=False,\n",
    "    print_every=100,\n",
    "    save_after_epoch=False,\n",
    "    save_as=\"model.pt\",\n",
    "    starting_epoch=1,\n",
    "    **kwargs\n",
    "):\n",
    "    # Initialize train /test DataLoaders:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Initialize Optimizer:\n",
    "    if type(optimizer) == str:\n",
    "        assert(optimizer in torch.optim.__dict__), (\n",
    "            \"optimizer must be one of the optimizers available in the \" +\n",
    "            \"torch.optim module, e.g. 'Adam'\"\n",
    "        )\n",
    "        optimizer = torch.optim.__dict__[optimizer]\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if not epochs_to_run and not batches_to_run:\n",
    "        epochs_to_run = 1\n",
    "\n",
    "    if epochs_to_run:\n",
    "        _train_for_n_epoches(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            optimizer,\n",
    "            epochs_to_run,\n",
    "            verbose,\n",
    "            print_every,\n",
    "            save_after_epoch,\n",
    "            save_as,\n",
    "            starting_epoch\n",
    "        )\n",
    "    else:  # if batches_to_run:\n",
    "        _train_for_n_batches(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            optimizer,\n",
    "            batches_to_run,\n",
    "            verbose,\n",
    "            print_every\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Fine Tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: loss = 27.30759620666504\n",
      "Batch 20: loss = 25.67279052734375\n",
      "Batch 30: loss = 23.840852737426758\n",
      "Batch 40: loss = 22.456920623779297\n",
      "Batch 50: loss = 20.551298141479492\n",
      "Batch 60: loss = 19.559797286987305\n",
      "Batch 70: loss = 18.30556869506836\n",
      "Batch 80: loss = 18.561481475830078\n",
      "Batch 90: loss = 18.1420841217041\n",
      "Batch 100: loss = 17.304948806762695\n",
      "Batch 110: loss = 16.80621910095215\n",
      "Batch 120: loss = 16.02007293701172\n",
      "Batch 130: loss = 15.240540504455566\n",
      "Batch 140: loss = 14.348015785217285\n",
      "====================\n",
      "After 1 epoch(s):\n",
      "  loss = 15.570582103729247\n",
      "Batch 150: loss = 13.365063667297363\n",
      "Batch 160: loss = 12.771071434020996\n",
      "Batch 170: loss = 11.247194290161133\n",
      "Batch 180: loss = 11.675347328186035\n",
      "Batch 190: loss = 9.92550277709961\n",
      "Batch 200: loss = 9.5925931930542\n",
      "Batch 210: loss = 9.69677448272705\n",
      "Batch 220: loss = 13.689676284790039\n",
      "Batch 230: loss = 9.222021102905273\n",
      "Batch 240: loss = 7.9541850090026855\n",
      "Batch 250: loss = 10.609424591064453\n",
      "Batch 260: loss = 9.229236602783203\n",
      "Batch 270: loss = 8.458810806274414\n",
      "Batch 280: loss = 6.607000350952148\n",
      "After 2 epoch(s):\n",
      "  loss = 8.430783414840699\n",
      "Batch 290: loss = 7.99317741394043\n",
      "Batch 300: loss = 7.62313175201416\n",
      "Batch 310: loss = 7.536841869354248\n",
      "Batch 320: loss = 6.482236385345459\n",
      "Batch 330: loss = 8.27229118347168\n",
      "Batch 340: loss = 6.904493808746338\n",
      "Batch 350: loss = 6.140580654144287\n",
      "Batch 360: loss = 9.150201797485352\n",
      "Batch 370: loss = 6.752841472625732\n",
      "Batch 380: loss = 5.931519985198975\n",
      "Batch 390: loss = 6.539213180541992\n",
      "Batch 400: loss = 7.079113006591797\n",
      "Batch 410: loss = 7.8425469398498535\n",
      "Batch 420: loss = 7.259895324707031\n",
      "After 3 epoch(s):\n",
      "  loss = 7.3675093015035\n",
      "Batch 430: loss = 11.951847076416016\n",
      "Batch 440: loss = 5.685118198394775\n",
      "Batch 450: loss = 5.187355995178223\n",
      "Batch 460: loss = 5.4723968505859375\n",
      "Batch 470: loss = 7.442089080810547\n",
      "Batch 480: loss = 6.737175464630127\n",
      "Batch 490: loss = 6.4940948486328125\n",
      "Batch 500: loss = 6.999383926391602\n",
      "Batch 510: loss = 7.249527454376221\n",
      "Batch 520: loss = 5.983572006225586\n",
      "Batch 530: loss = 7.543515205383301\n",
      "Batch 540: loss = 6.640780925750732\n",
      "Batch 550: loss = 10.36477279663086\n",
      "Batch 560: loss = 5.394728183746338\n",
      "After 4 epoch(s):\n",
      "  loss = 6.727892939249674\n",
      "Batch 570: loss = 5.777163982391357\n",
      "Batch 580: loss = 7.131552696228027\n",
      "Batch 590: loss = 6.3644866943359375\n",
      "Batch 600: loss = 6.2029032707214355\n",
      "Batch 610: loss = 9.081936836242676\n",
      "Batch 620: loss = 5.108344078063965\n",
      "Batch 630: loss = 5.297098636627197\n",
      "Batch 640: loss = 5.487334728240967\n",
      "Batch 650: loss = 4.677786827087402\n",
      "Batch 660: loss = 6.06415319442749\n",
      "Batch 670: loss = 7.204761981964111\n",
      "Batch 680: loss = 6.090863227844238\n",
      "Batch 690: loss = 6.073877334594727\n",
      "Batch 700: loss = 6.388937950134277\n",
      "After 5 epoch(s):\n",
      "  loss = 6.537152481079102\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=pipeline.model,\n",
    "    train_data=TrainingData(X, Y),\n",
    "    # Adam leads to NaN / infinite model weights after first gradient step:\n",
    "    optimizer=\"SGD\",\n",
    "    epochs_to_run=5,\n",
    "    batch_size=2,  # Needs to be very small to not run out of memory\n",
    "    # Learning Rates:\n",
    "    # 1e-5 = Too high; very bad results (\"scrambles\" model's brain)\n",
    "    # 1e-6 = Okayish; qualitatively reasonable results, but compilability and\n",
    "    # test pass rate worsens on average with each additional epoch of finetuning\n",
    "    # 1e-7 = TBD\n",
    "    learning_rate=1e-7,\n",
    "    verbose=True,\n",
    "    print_every=10,\n",
    "    save_after_epoch=True,\n",
    "    save_as=MODELS_PATH + \"fine_tuned_llama3_smaller_LR_after_{epoch_number}_epoch.pt\",\n",
    "    starting_epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save:\n",
    "# torch.save(pipeline.model.state_dict(), MODELS_PATH + \"fine_tuned_llama3_smaller_LR_after_5_epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Load:\n",
    "# pipeline.model.load_state_dict(torch.load(MODELS_PATH + \"fine_tuned_llama3_smaller_LR_after_5_epoch.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
